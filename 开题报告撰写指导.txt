保研加分小助手项目开题报告
第一章 绪论
1.1 研究背景
推荐优秀应届本科毕业生免试攻读研究生（俗称“保研推免”）是在高校中选拔培养拔尖人才的重要措施。但推免资格的评定涉及复杂的学术成果、竞赛奖项和综合表现评分体系，对申请材料的收集与评分往往依赖人工处理[1]。以厦门大学信息学院为例，过去推免工作要求学生填写成绩加分表并提交各种证明材料，由学院组织人工核算加分并公示排名[2]。这种人工流程不仅繁琐，易出错，而且学生难以及时了解自己的评分情况，增加了信息不对称和不公平的风险。随着人工智能和软件技术的发展，将OCR识别、自然语言处理等技术应用于推免材料的自动识别与评分具有现实意义，可以提升工作效率并确保评分过程的公正透明。
1.2 研究内容
本项目旨在设计并实现“保研加分小助手”网站系统，围绕推免细则及评分要求，对学生提交的申报材料进行自动化的收集、整理、识别和加分计算，并生成排名。系统前端采用Vue 3框架和Element Plus组件库构建友好的用户界面，后端采用Kotlin语言的Spring Boot框架实现业务逻辑，通过RESTful API供前端调用，实现前后端分离的架构[3]。
系统的核心功能包括：用户注册登录、个人成绩及成果信息填报、材料证据的OCR识别与内容提取、科研论文和竞赛奖项等成果的自动分类评估、综合成绩计算与排名、管理员审核管理等。通过对接第三方OCR和NLP接口，实现对证书扫描件、论文信息等非结构化数据的自动提取和分析，减少手工录入和判断的工作量。同时，系统将内置推免评分算法，以高校推免政策为依据对各类成果赋分，帮助师生快速得到加分明细和总分，从而辅助推免资格遴选决策。
1.3 研究现状
目前，大多数高校的推免加分审核仍以线下或半线上方式进行：如学生填写Excel表自评、提交PDF材料邮件汇总，由学院老师或学生助理小组手工复核汇总[1]。这种方式效率较低，且人工评分难免出现遗漏或主观误差。一些高校开始尝试开发推免管理系统，如厦门大学在2025年已上线推免生管理系统，实现在线填报和材料上传[4]。然而，这类系统通常仅提供信息提交与流程管理，对成果自动识别和智能评分支持不足，仍需依赖管理人员逐条核对打分。
另外，在学生综合素质评价领域已有一些探索：如有开源项目支持自定义配置评分规则、自动计分的大学生综合测评系统[5]。学术研究方面也提出利用AHP等方法优化学生综合评价模型，提高客观性和智能化水平[6][7]。这些现有工作为本项目提供了借鉴，表明利用软件系统实现学生评价的自动化与智能化是可行且有价值的，但目前尚缺针对推免加分细则高度定制的解决方案。
1.4 研究目的
本项目的目标是开发一套高效、准确、可扩展的推免加分辅助系统，以解决当前推免材料审核过程中存在的痛点。通过该系统，学生可以方便地上传成绩单、论文、竞赛证书等材料，系统自动识别有效信息并根据学院推免文件进行加分计算，让学生实时了解自己的推免综合成绩；学院管理员可以通过系统批量审核学生的材料和分数，快速生成排名，提高工作效率并降低错误率。系统还将提供透明的评分依据说明，增强推免工作的公开、公平、公正。
归根结底，本研究希望利用先进的OCR和NLP技术结合严谨的软件工程设计，构建出一个兼具学术严谨性和技术实用性的推免加分小助手，为高校的人才选拔提供科技助力。
2 需求分析
2.1 研读推免政策文件
根据厦门大学信息学院最新修订的推免实施细则和综合成绩计算办法（2025年2月版），推免综合成绩由学业综合成绩和考核综合成绩两部分构成[8]。学业成绩一般占80%，考核综合成绩包含学术专长成绩（15分）和综合表现成绩（5分）[8]。其中，学术专长成绩又细分为科研论文、发明专利、学科竞赛、创新训练等类别，每类有具体的加分标准上限；综合表现成绩涵盖志愿服务、荣誉称号、学生干部任职、参军入伍、体育比赛等方面，也各有明确加分规则和上限[6][7]。
此外，学院设置了“特殊学术专长”推免通道：如果学生在本科期间满足顶尖论文发表（如Nature/Science等）、重大竞赛获奖等条件，可通过教授联名推荐和公开答辩获得学术专长满分加15分的认定[9][10]。所有加分成果的取得时间截止推免当年8月31日，超过一定门槛的重修课程或处分情况会影响推免资格[11]。
这些文件形成了本系统评分算法和逻辑判定的依据，要求系统在需求分析阶段对评分细则进行全面梳理并转化为可实现的功能模块。
2.2 存在的问题
在当前人工操作流程下，数据收集和评分核算存在多重问题：首先，学生提交材料格式不统一，审核者需要花大量时间整理归类证据，容易出现遗漏或顺序错误。其次，人工比对成果与加分规则费时费力，遇到规则更新可能理解不一致，造成评分误差或不公。再次，学生对自己的加分情况缺乏及时反馈，难以及早发现材料不足或评分争议，后期异议处理成本高。最后，大批量学生数据的统计排名全靠人为处理，效率低且难以及时发布结果[2]。这些痛点凸显出引入信息系统的必要：通过自动化工具帮助解析材料、套用规则，可以减少人工重复劳动，让工作人员从繁琐计算中解脱，专注于异常情况的复核；对于学生，则提高了过程透明度和参与感。
此外，考虑到推免政策可能调整、评分规则可能变化，如果缺乏系统支持，每年都要培训新的学生助理小组手工核算，存在维护难度。而本项目将把规则配置和算法封装在网站中，后续若规则更新仅需调整配置或算法模块，即可应用于新的评选年度，增强了可持续性和可维护性。
2.3 用户画像与调研
项目的目标用户主要分为两类：推免申请学生和学院管理员/评审老师。学生用户一般为大三或大四应届本科生，具有一定的计算机使用基础，希望简明快捷地完成推免申请。
通过调研发现，学生群体关注的是系统易用性和正确性：界面友好、指引清晰，能一步步提交个人信息和成果材料；系统能准确识别出上传材料中的要素（如论文作者、刊物级别，竞赛名称、奖项等级等），并给出对应的加分值和依据，使学生对自己的综合成绩心中有数。因此系统需提供良好的用户体验：用户指引（如模板下载、示例填写）、表单自动校验，以及在提交后即时呈现加分明细以供核对。
管理员用户包括学院负责推免的老师和助理人员，他们关心的是系统的可靠性和高效审核支持。调研表明管理员希望系统能够批量呈现学生成绩排序、各加分项来源，以及提供材料查看和人工调整接口。当OCR/NLP自动识别结果有疑虑时，管理员应能方便地查看原始材料（例如点击查看证书扫描件）并编辑校正。
同时，需要权限控制：普通学生只能查看和编辑自己的数据，管理员可以管理所有数据。结合这些画像，系统设计时将采用角色权限机制区分不同用户界面和功能；在学生端强调流程引导和反馈，在管理端注重数据汇总和审核便捷。
2.4 市场与竞品分析
当前高校推免评选多属于校内管理事务，没有专门的商业软件产品，通常由各校自行开发或借用通用的学生综合评价系统。国内一些高校的信息化系统提供了综合测评或推免模块，但功能侧重于流程管理而非智能评分。例如，厦门大学推免生管理系统主要用来收集申请信息和材料[4]，最终的评分仍需线下核算后导入系统。
市面上也有面向高校教务的综合素质评价系统，其特点是可灵活配置评价指标和权重，实现一定程度的自动打分[12]。如开源项目“cqes4cs”支持自定义加分规则、自动计分，帮助高校开展学生综合素质评价[12]。这类系统技术架构上常采用前后端分离模式（Vue+ElementUI前端，Spring Boot后端），支持Excel导入导出，性能和功能已被验证[5][13]。
然而，推免加分细则具有明显的校本特色和复杂业务规则，一般综合评价系统未必直接适用。我院推免政策涉及特定会议期刊目录、特定竞赛清单等个性化要求，现有产品需要二次开发配置才能满足。本项目在分析竞品优缺点的基础上，选择自主设计，以满足定制需求。同时借鉴竞品的优点：规则配置（考虑将部分评分规则参数化，便于日后调整）、批量处理（一次处理多名学生的数据，提高效率）、可追溯性（保留每项成绩来源供审核）等，确保我们的系统在功能针对性的同时，具备通用评价系统的良好特性。
2.5 相关论文分析
学术界对学生成绩与综合素质评价信息系统已有一些研究。比如，霍聪聪等人设计了新工科大学生综合素质智能评价系统，采用层次分析法（AHP）确定指标权重，使用可视化手段展示学生能力结构，以期克服传统评价的主观片面问题[6][7]。这表明通过算法模型提高学生评价客观性是趋势之一。
对于推免工作，一些高校在教务系统中集成了推免模块，实现成绩排名和资格审核在线化[14]。但特定到学术成果加分自动核验，目前公开资料较少。我们推测可以借鉴文档智能解析的相关研究：如阿里云的文档智能API能提取文档层级结构、文本和键值等信息，实现对复杂PDF材料的自动解析[15][16]。再结合自然语言处理，如命名实体识别和文本分类技术，可用于识别论文题目中的期刊名、竞赛证书中的赛事级别等关键信息[15]。
总的来说，相关研究启示我们应将规则驱动和机器智能相结合：一方面把学校明确的评分规则以程序算法固化，另一方面利用OCR/NLP降低人工录入负担和错误。通过充分调研文献和现有系统案例，我们明确了需求重点：确保系统对各类加分项判断准确、流程高效且可解释（每个加分都有依据说明），这也正是本项目在需求阶段确立的设计准则。
2.6 网站功能需求总结
综合以上分析，提炼本系统的主要功能需求如下：
- 用户注册与登录：提供学生和管理员两类账户注册登录功能，采用统一身份认证与权限控制机制。学生注册需绑定学号等信息，管理员账户由系统预先配置。
- 学生信息维护：学生登录后填写个人基本信息和学业成绩（如平均绩点、英语水平成绩等）。学业成绩也可由系统管理员批量导入。
- 成果材料上传：学生按类别提交各项加分成果的证明材料（如论文PDF或截图、竞赛获奖证书扫描件、专利证书等）。允许上传图片或PDF格式，并针对每项材料填写必要的元数据（如成果名称、取得日期等）以辅助识别。
- OCR文字识别：系统对学生上传的图片/PDF进行文字识别，提取关键信息字段。例如识别证书中的竞赛名称、奖项等级、颁发单位，论文版面中的论文标题、作者、刊物名称等[17]。通过调用第三方OCR API实现高精度识别[18]。
- 成果自动分类与加分：利用NLP和规则引擎，将OCR提取的信息与推免加分规则匹配：如根据刊物名称判断论文属于A类/B类/C类目录，对应不同基准分值[9]；根据竞赛名称匹配学院竞赛库中的级别（A+/A/A-）并识别获奖等次赋分[11]。系统应支持复杂规则计算，例如团队奖项分配（按队员排名比例分配分值）[12]、成果数量上限（C类论文最多计2篇，超过不再加分）等。特殊学术专长申请者经管理员标记后，系统直接赋满分15分给其学术专长项。
- 加分明细与成绩查询：学生提交材料后，系统实时计算其各项加分及总成绩，提供明细清单页面。比如显示：“学术论文A类1篇，加10分；国家级A类竞赛二等奖1项，加10分；志愿服务时长250小时，加1分；…”等，使用户清楚每项来源与得分。引用规则出处或政策文件片段作为说明，以提高结果可信度。
- 管理员审核：管理员登录后，可查看所有学生列表及其当前综合成绩排名。针对每个学生，管理员进入审核界面：核对系统识别的每项成果及分值，支持查看原始材料（预览图片/PDF）以人工确认[19]。若系统分类有误，管理员可调整分类或分值（需记录原因）。管理员审核通过后锁定该生成绩，进入最终排序。
- 排名与公示：系统按照各专业或全院，将学生按推免综合成绩高低排序生成排名表。支持筛选（如只看符合资格者）和导出功能。对最终确定的推免名单，可由系统生成公示页面或报告，并确保未经公示的数据不被正式提交[20]。
- 通知与日志：为关键节点提供消息通知，例如学生提交成功提示，管理员审核结果通知等。系统记录所有修改操作日志，以备查询溯源（尤其管理员改动分值时）。
- 配置与维护：提供管理员后台管理一些基础数据，例如：推免竞赛项目库、学术期刊目录（附件1和附件2内容）可在系统中更新维护；评分规则参数（如各类别分值上限、各角色得分比例等）在代码或配置中易于修改。支持数据备份和Docker部署维护，保证系统稳定运行。
上述功能需求将指导系统的设计与实现，确保最终产品能够全面满足推免加分工作的业务需要。
3 系统分析
3.1 可行性分析
本项目在技术上具有较高可行性。一方面，所需实现的OCR文字识别和NLP分类等功能，可以借助成熟的云服务API完成，无需完全自主训练模型。例如阿里云提供通用文字识别OCR接口，能对多种类型文档高精度提取文本[18]；其自然语言处理服务支持文本实体抽取和分类等功能[15]，可用于识别材料中的赛事名称、刊物级别等敏感字段。这些云服务有完善的SDK和文档，可快速集成[18][21]。另一方面，前后端分离的Web系统构建对团队也是可掌控的：Vue.js和Element Plus在社区有大量示例，Spring Boot结合Kotlin开发能大幅减少模板代码量，提高开发效率[22]。同时，评分规则的算法相对明确，根据文件条款编写逻辑即可，重点是处理好各种分支情况与上限控制，这可通过充分测试来保障正确性。
经济上，由于是校内项目，服务器资源和API调用费用在可控范围（学校或学院可提供一定支持，如阿里云教育优惠等），不会有商业运营压力。进度上，我们规划了明确的里程碑（详见工作计划部分），采用模块化并行开发策略，预计在可接受的时间内完成原型和优化。
项目潜在风险主要在OCR准确率和规则复杂度：OCR可能对某些模糊图片出错，NLP可能无法100%正确分类所有竞赛名称。对此方案是提供人工校验和规则不断完善机制，即管理员复核纠错，以及对出现的新情况及时更新系统规则库。
总体而言，依托现有成熟技术栈和团队经验，本系统的开发是可行的，预期能够按质按量交付。
3.2 功能分析
在正式设计前，有必要将需求功能细化为模块，以明确系统必须支持的功能点和交互流程。概括而言，系统分为前端展现层和后端服务层两部分，各自包含若干功能模块：
前端展现层模块：
- 用户登录/注册模块：提供账号注册、登录认证及退出功能，调用后端认证API，保存会话状态。
- 学生端功能模块：包括个人信息填报页面、成果上传页面、加分明细查看页面、申请状态查看页面等。支持交互如表单输入校验、文件上传预览、实时分数显示等。
- 管理员端功能模块：包括学生列表与搜索、单个学生审核界面、推免排名展示界面、系统配置界面等。要求界面可快速检索定位学生，直观呈现其材料及分数，提供操作按钮（通过/退回/修改）。
- 公共组件：如导航菜单、消息通知弹窗、文件预览对话框、加载动画等，提高UI一致性和复用性。
后端服务层模块：
- 用户管理模块：负责用户账户的注册、登录认证、角色权限校验。采用JWT或Session机制维护会话，确保不同角色访问受限资源时进行权限拦截。
- 材料存储与OCR模块：接收前端上传的文件，将文件存储在服务器（或云存储），并调用OCR API识别文字内容。对识别结果进行基础清洗（如去除空白、转换格式），返回可解析的文本给后续模块。
- 成果识别与评分模块：这是系统核心逻辑。包含若干子组件：
  - 论文成果识别：根据OCR文本或用户输入，识别论文名称、刊物名称、作者位置等，匹配学院A/B/C类期刊会议目录[8]，判断得分（考虑作者排序加权、学校单位要求等）。
  - 竞赛成果识别：解析竞赛证书文本，匹配竞赛库确定竞赛级别和类别（A+国家级/省级等），并识别奖项等次赋分[11]。若团队赛，确定团队规模和名次以计算分配比例。
  - 专利及创新项目识别：识别专利授权号/时间及排名，按规则计2分（或相应比例）[12]；创新训练项目根据国家/省/校级结题，给定不同基础分值。
  - 综合表现识别：解析志愿服务时长（汇总时长并按每2小时0.05分计算，上限1分等）及志愿表彰级别[23]；解析荣誉称号级别（校级0.2、省级1、国家级2，考虑集体荣誉得分减半）等；学生干部任职则根据职务系数和考评分计算得分[24][25]；参军服役年限、体育竞赛奖项按照规则赋分。
  - 规则引擎：综合上述识别结果，应用所有上限和不累加规则，例如：C类论文最多算2篇、同一作品不同比赛取最高分、不同时兼任多职务的只取最高年度分等[26]。确保最终每类加分不超规定上限，所有加分项总和控制在学术专长15分、综合表现5分的范围。
- 成绩计算与排名模块：获取每个学生的学业成绩（按GPA或排名折算分），加上其学术专长分和综合表现分，计算出推免综合成绩=学业成绩*0.8 + 专长分 + 综合分[8]。将结果存入数据库。提供按专业或全院排名的查询接口。
- 审核与反馈模块：实现管理员对学生各项数据的审批流程。包括标记审核状态（未审/通过/退回）、对退回的申请生成反馈信息供学生查看，以及锁定最终成绩等功能。此模块还需实现日志记录，保存每次管理员改动详情。
- 配置管理模块：提供基础数据的维护接口，如增删改竞赛项目、期刊目录条目，以及修改某些评分参数（可以通过配置文件或简单后台界面来完成）。这样当政策调整时，管理员有一定调整能力。
模块划分有助于明确各部分职责和接口，在实现时将按照这一划分进行代码组织和任务分配。
3.3 用户视角流程分析
从普通学生用户的角度，其使用流程可分为以下几个阶段：
1. 账号注册与资格确认：学生首次使用需注册账号，输入学号、姓名、专业等并验证身份。登录后系统会提示他们阅读推免申请条件，确认是否符合基本资格（例如无重修超过3门次、英语达标等[11]）。若不符合，系统友好告知不能继续申请。
2. 填写学业信息：学生进入个人信息页面，核对从教务系统导入的成绩数据或自行录入必要信息，如专业排名、英语四级/六级成绩等。系统自动校验格式，例如四级必须≥500分或六级≥425分才满足外语要求[27]。若不满足，系统警示但允许暂存，以备8月31日前补充成绩[28]。
3. 提交成果材料：学生逐项添加自己的学术成果和综合表现。系统提供“添加论文/添加竞赛/添加荣誉”等入口，每个入口是一个表单+文件上传组件。学生根据提示填写关键信息（如论文标题、刊物、发表日期；竞赛名称、级别、奖项等），并上传对应证明文件图片/PDF。每上传一项，系统即时调用OCR和规则引擎计算出预估加分，在页面上展示。例如：“已识别XXX竞赛国家二等奖，预计加10分”。学生可持续添加直至所有成果输入完毕。系统对常见错误给予提醒，例如重复上传同一项目、多次参加同赛事取最高分、不支持的不在竞赛库项目将不计分提示等。
4. 查看加分明细：学生在“我的加分”页面可以看到系统列出的所有已录入成果及对应得分、学业成绩折算分，以及当前总分和排名（若开放查询排名）。如果某项识别有误，学生可在提交前手动修改相关信息字段并重新计算，或删除后以其他类别重新添加。该页面旨在让学生对自己的推免综合成绩做到心中有底，并核实系统识别是否准确。
5. 提交审核与结果查询：在学院规定的申请截止日前，学生确认无误后提交审核。提交后数据即锁定进入管理员待审列表。学生可在个人主页看到自己的状态（如“审核中”）。管理员审核期间若发现问题将退回，学生会收到通知并可在系统查看具体反馈（需要补充哪些材料或信息）并进行修改后再次提交[10]。审核通过后，学生状态显示“审核通过”，并在公示期可以查询到最终成绩排名结果。如果学生最终获得推免资格，系统可以提示下一步去教育部推免服务系统填报志愿等事项[29]。整个流程中，学生端注重交互体验和信息透明，例如在重要阶段通过站内消息或邮件告知学生。
通过上述流程分析，可以看出系统需满足学生多次填报、实时反馈、自主纠错的需求，让他们有清晰指引完成申请。
3.4 管理员视角流程分析
学院推免工作小组或授权管理员使用本系统时的流程则注重批量处理和严谨审核：
6. 账号分配与登录：管理员账户由系统预先创建分配（可与学校统一认证对接）。管理员登录后进入后台管理界面，默认看到待审核学生列表。
7. 材料初审：列表中呈现所有已提交的申请条目，包括学生姓名、专业、当前综合成绩及各项分值、是否有特殊学术申请等简要信息。管理员可按专业或提交时间过滤，并逐个进入学生详情审核页面。
8. 详细审核页面：在某学生审核界面，系统展示该生所有信息：学业成绩及排名情况，每一项加分条目（按类别分组）及其证明材料链接和系统给出的得分。管理员需要逐条核对：点击材料预览，看OCR提取结果与证书内容是否一致，判断系统分类和评分是否正确。例如如果某论文刊物被系统识别为B类但管理员知道是A类，可直接在界面上修正分类，系统即时重新计算分值；如果某竞赛不在认定列表，应将该条标记为不计分或移至“其他经历”不计分区。管理员的所有改动系统应自动记录。对于特殊学术专长申请，管理员需要额外确认教授推荐信等材料是否齐全，然后将其标记为“通过特殊专长”（使该生学术专长直接15分满分）。管理员还检查材料真实性和规范性，若发现疑似造假或不符合要求的材料，可退回申请并填写原因说明，让学生补交或取消申报[10]。
9. 审核决议：管理员审核完所有条目后，对该生做出决议：通过或退回。如通过，则状态更新为已审核，记录最终确认的各项得分（管理员改过的以最终为准）进入排名；如退回，则系统要求填写退回理由，并触发通知学生修改。管理员可在界面上保存当前审核进度，以便支持分批审核、多人审核协作（需要时可让多个管理员账号分别处理不同专业学生）。
10. 排名汇总与名额分配：当所有待审申请都处理完毕，管理员切换到“排名”功能。系统按各专业排序列出综合成绩排名，并标注学院可推荐名额。管理员可调整名额分配或查看拟推荐名单。系统根据名额自动筛选出各专业前N名作为初步名单[30]。管理员可导出这一名单以及所有成绩明细备份。公示阶段，系统可以提供一个只读界面用于校内公示，显示入选者成绩和名次（无个人敏感信息）。若公示期间有异议，管理员能查询日志追溯评分依据，必要时进行解释或调整（调整亦需审批流程）。
11. 数据上报与维护：最终推免名单确定后，管理员可以通过系统生成报送教务处或教育部系统所需的数据文件（如Excel或直接API对接）。之后系统转入维护状态，管理员可以重置或清理部分数据为下一年度准备，并对新政策修改配置等。
管理员端流程强调准确、可追溯、效率。因此系统必须有友好的管理UI来展示复杂数据，并提供人工override机制[19]。同时安全上要保障普通学生不能越权访问管理功能，管理员操作应有日志以防止暗箱。分析以上流程，可以进一步校验我们的功能设计是否周全，例如材料预览、意见填写、日志等都是必需的。
3.5 风险分析
在项目开发和实施过程中，需要识别并提前制定措施来降低各种潜在风险：
- 技术风险：主要在于OCR和NLP效果。如果OCR对低质量扫描件识别错误率高，可能导致加分计算错误。缓解措施：选择业界知名OCR服务（如阿里云OCR，在印刷文本识别上精度较高[18]），并要求学生上传清晰扫描件；对于关键字段（如竞赛名称），结合正则或关键词库做二次校正。NLP分类风险在于新竞赛或新期刊名称无法准确匹配。对此可设立人工确认流程：凡是系统无法高置信度识别的材料（比如匹配不到竞赛库的名称），标记为“需人工判定”，由管理员手工选择正确类别。
- 规则变更风险：推免政策可能调整，如加分标准或名额分配策略变化。若系统设计写死规则，将来维护成本高。解决办法：在系统设计中引入配置驱动思想，将竞赛项目库、期刊目录和一些参数（分值、上限）做成可配置的形式，尽量减少硬编码。并编写详细文档，方便后续维护者根据新政策调整代码。
- 进度风险：项目功能广泛且技术栈新，团队可能在集成上遇到挑战。缓解：采用敏捷开发，划分里程碑，每两周一个小版本，优先实现核心功能（如基本提交和评分），逐步增加复杂功能。同时做好人员分工和每日站会沟通，及时发现偏差。预留一定缓冲时间用于联调和测试。
- 安全风险：系统涉及学生成绩和个人成果等敏感信息，要注意访问控制和数据安全。措施：采用JWT认证并在后台对每个接口做权限检查；重要操作（如管理员改分）要求二次确认；所有上传材料存储采取访问限制，防止未授权用户通过URL直接访问他人文件。部署时通过HTTPS和Nginx配置防护，保证数据传输和存储安全。
- 用户接受度风险：学生和老师对新系统使用不熟悉可能产生抵触或操作失误。应对：在上线前进行培训和手册编写，提供模拟数据环境供练习；系统界面尽量简洁直观，在复杂规则方面给予说明提示，让用户了解系统判分的依据和过程，从而建立信任。在正式应用的首年安排人工核对抽查，确保系统结果准确，与人工结果差异可解释，以获取用户信心。
- 系统性能风险：在提交高峰期（截至日期前夕）或管理员并行审核时，可能出现大量并发请求。虽然用户规模不大（信息学院应届生数百人），但OCR接口调用需要考虑限流和响应延迟。对策：本地部署队列机制，对于OCR请求采取异步处理并提示稍后查看结果，避免阻塞用户操作；数据库查询优化、分页显示以提高响应；必要时可以缓存一些常用数据（如竞赛项目库加载）。
- Docker部署风险：容器化部署要求团队掌握Dockerfile和Compose配置。为防止部署阶段出问题，我们会提前编写并测试Docker镜像，在测试环境完成从启动到访问的全流程验证，确保生产环境能一键部署成功。同时利用CI工具在代码合并时自动构建镜像，发现问题及时修复。
通过上述风险识别和措施制定，我们将把风险控制在可接受范围，确保项目按计划进行并平稳上线运行。
4 系统设计
4.1 系统架构拆解与模块设计
根据需求和分析结果，我们采用分层和模块化架构进行系统设计，确保清晰的职责划分和可维护性。系统整体采用典型的B/S架构（Browser/Server），前后端分离部署[31][32]。主要的设计考虑如下：
- 前端架构：使用Vue 3框架构建SPA（单页应用），利用其组件化机制组织界面。引入Pinia作为状态管理库，用于跨组件共享用户会话、基本数据等。Pinia特点是直观、类型安全且模块化设计良好，开发体验轻量高效[33][34]。UI组件方面采用Element Plus（Vue 3的Element UI重构版）提供成熟的交互组件，如表单、表格、对话框、上传控件等，减少基础样式工作量。Element Plus是基于Vue3的组件库，包含丰富的基础组件，能方便实现各式界面需求[35]。整体前端采用单页面多组件路由设计，确保不同功能有独立组件模块：登录注册组件，学生首页（个人信息+导航），成果上传组件（根据类别不同子组件），加分明细组件，管理员首页（统计概览），审核列表组件，审核详情组件，排名公示组件 等。 前端通过Axios调用后端RESTful API，实现数据交互。界面风格简洁明了，注重响应式布局以兼容不同终端（PC端为主，适当兼容移动端浏览查看）。
- 后端架构：采用Spring Boot框架搭配Kotlin语言开发RESTful服务。Spring Boot提供内嵌服务器、自动配置等简化了开发部署过程，Kotlin使代码更加简洁紧凑[22]。Kotlin的空安全和默认参数等特性让我们可以减少大量样板代码，如替代Java的构建者模式，通过默认参数快速构造对象[22]。Spring Boot与Kotlin深度集成，Spring官方也提供Kotlin扩展使代码更具表现力、更简洁[36]。项目采用分层架构和领域驱动设计思想组织代码：主要分为接口层(Controller)、应用服务层 (Service)、领域模型层 (Domain) 和基础设施层 (Infrastructure)[37][38]。
  - 接口层：定义RESTful API的控制器，映射URL到对应服务。例如UserController, StudentController, AdminController等，负责接收请求、参数校验和返回统一格式结果（JSON数据）。
  - 服务层：实现具体业务逻辑，是系统核心。如ScoreService封装成绩计算流程，OCRService封装对接OCR API调用逻辑，RuleEngine组件实现规则判断等。服务层调用领域模型和仓库完成业务操作。
  - 领域模型层：定义核心业务对象和规则。如实体类Student, Achievement, Paper, CompetitionResult等，包含其属性和简单业务方法。采用JPA或MyBatis访问数据库，Repository接口隶属此层，提供实体的持久化CRUD功能。
  - 基础设施层：封装与外部的集成，如调用云OCR/NLP API的客户端、发送邮件短信的工具类，文件存储的服务，以及应用配置等。不直接参与业务逻辑，但为上层提供通用支持。这种领域驱动的包结构遵循“先业务，后技术”的分包原则[24]。即代码首先按业务领域归类，例如package edu.xmu.pushrec.competition 包含与竞赛成果有关的服务、实体、仓库；edu.xmu.pushrec.paper 包含论文相关逻辑；edu.xmu.pushrec.user 包含用户账户相关等等。在每个业务包内部再按技术分层，如子包domain, service, controller等划分。这种结构让业务模块清晰直观，方便开发者按业务理解代码，并具备易扩展、易维护的优点[25]。
- 数据库设计：采用MySQL关系数据库，所有主键字段使用UUID（Char(36)或二进制16字节）作为主键，以避免并发下的序列冲突并方便不同表间引用。数据库模型围绕推免业务建立，主要实体及ER关系如下：
  - 用户表（User）：字段包括用户ID、姓名、角色（student/admin）、密码哈希、专业等。如果与学校账户集成也可只存部分信息。
  - 学生成绩表（AcademicRecord）：字段包括学生ID、专业排名/绩点、英语成绩、重修门次等，用于校验和计算学业成绩。
  - 成果基类表（Achievement）：抽象出通用属性，如成果ID、学生ID、类型(enum：论文/竞赛/专利/荣誉/社会工作等)、标题/名称、取得日期、状态（待审/通过/退回）、得分等。也可设计为多张表：PaperAchievement, CompetitionAchievement等继承Achievement，各自加特有字段。为简单起见，本项目倾向于单表存多类别加一个类型字段，这样便于统一管理查询；特有属性可用JSON字段存储或额外关联表。
  - 论文详情表（PaperDetail）：包括成果ID（对应Achievement表）、刊物名称、是否CCF推荐A/B类、影响因子、作者排序、导师ID等。竞赛详情表（CompetitionDetail）：包括成果ID、竞赛名称、级别(A+/A/A-)、奖项等级、一队人数、本人成员排名等。类似的还有PatentDetail, HonorDetail等，用于存放各类别特有的信息。通过这些子表可以精细存储信息，同时便于规则计算。
  - 综合成绩表（TotalScore）：可选表，为了冗余存储每年每个学生最终的推免综合成绩和排名，字段有学生ID、总分、排名、年度、是否推荐资格等。也可不建表，动态计算排序后临时展示，但建表有助于保存历史记录和对比分析。
  - 基础数据表：如CompetitionInfo（竞赛项目库，字段：名称、级别、类别）、JournalInfo（期刊会议库，字段：名称、分类级别）、HonorInfo（荣誉称号级别配置）等。这些用于配置学院认可的赛事/期刊范围，系统判定时会查这些表匹配。
  - 推荐名额表（Quota）：存储每年学院各专业推免名额配额，以及特殊政策名额等，可供排名筛选用。
- 数据表之间联系：
  - User(一) - (多) Achievement（学生与其提交成果）；
  - Achievement(一) - (一) PaperDetail/CompetitionDetail等（通过成果ID作为外键）；
  - User(一) - (一) AcademicRecord。
这样可以根据需要JOIN查询，也可以在应用层通过ID索引查询关联。数据库ER图将在详细设计文档中绘制，以确保无冗余且一致性（例如Achievement类型和Detail表对应必须匹配）。 
- 其他网站架构设计：整体架构采用前后端分离的三层架构：浏览器前端 <-HTTP/JSON-> SpringBoot后端 <-JDBC/ORM-> MySQL数据库。前端部署在Nginx上，Nginx同时作为反向代理[39]：将API请求转发给后端微服务，将静态资源请求直接由Nginx的静态服务器响应（部署时把前端打包后的静态文件放置于Nginx指定目录）。这种架构确保前后端可以独立扩展部署，并利用Nginx的高并发处理和缓存提升性能[39]。Nginx也可以进行负载均衡，如果后端将来需要水平扩展可挂载多实例。鉴于当前项目规模，暂定后端为单体应用，但也按照领域模块组织以方便未来可能的微服务拆分[40]。系统对外开放统一的RESTful接口路径，如/api/students/...、/api/admin/...。按照RESTful风格设计资源URL，例如GET /api/students/{id}/achievements获取某学生全部成果；POST /api/students/{id}/achievements新增成果；管理员审批接口则如POST /api/admin/review/{achievementId}等。不在URI中加入版本号，遵循业界推荐做法保持API简洁一致，以通过良好设计和渐进式演进来支持版本升级而非维护多个版本[41][42]。Roy Fielding等专家认为在URL里使用/v1/是一种反REST的做法，“一个‘v1’就是对API客户竖中指”[41]。本项目采用无版本号的REST接口，若将来升级，通过向后兼容或HTTP头协商等方式处理，从而让客户端始终调用统一URI，不因版本切换而破坏兼容性[42]。
4.2 综合成绩算法设计
系统评分算法模块需要精确实现学院推免计算办法。设计上可采用规则表驱动+代码实现结合。比如定义枚举或配置：PaperScore{A:10, B:6, C:1}分值，CompetitionScore{A+:{1st:30, 2nd:15,...}, A:{1st:15,...}}等等，根据不同类别和获奖等级映射基准分，然后再应用额外规则（人数折算、上限限制等）。关键步骤包括：
1. 论文计分：根据期刊会议等级赋基准分（Nature等顶刊按两篇A类算20分[8]），然后按作者排序权重：如非导师作者的前两名按80%、20%分配[26]；如果学生是独立作者则100%。取每篇计算后的分值累加，C类论文最多取2篇[26]。例如某生有A类1篇+B类1篇，则得分=1080% + 680% = 8+4.8=12.8分（四舍五入或保留小数点后2位视政策）。
2. 竞赛计分：查表确定竞赛类别和级别，对应不同分值上限[26]。根据个人/团体和奖项：个人项目奖直接用规则表的1/3计算（比如国家级一等奖个人=301/3=10分）[26]；团体项目奖根据团队大小平均或按名次梯度[26]。系统实现时，可根据团队总人数n和要求：若特殊竞赛如“挑战杯”有固定比例(队长1/3，第二三1/4等)[26]，可在竞赛库配置；一般团赛则算法：n<=2每人1/3, n<=5每人平均=1/5, n>5仅前5计算每人=1/5。对同一作品多个奖只取最高分，不累计。
3. 创新创业项目计分：根据结题级别给分，且多级别不累加[26]。实现：记录每个项目ID，只取其最高级立项情况给分（如国家立项组长1分，组员0.3分），多个项目可累计但总和≤2。 
4. 综合表现计分：几个方面分别计算后相加，且不超过5：
- 志愿服务：读取志愿小时总数h，若h<200则0分，>=200则基准0分并((h-200)/20.05)计算额外分，max=1[23]。同时若有表彰奖励则取奖励对应分（校0.25、省0.5、国1）与工时分择优，不能累加[23]。实现时，可先算工时分，再算表彰分，取较高者，max1。
- 荣誉称号：遍历学生所有荣誉，按照最高级别赋分。若多项不同类别荣誉，同年度不累加，同学年只算最高[43]。设计上，可按年份分组，取每年最高分荣誉，加起来但<=2。 
- 社会工作（学生干部）：对于每个任职，获取对应系数，乘辅导员评分（百分制）得到分[38]。按任期长短（一年=全分，半学年=半分）计算。不同学年可以累加，但不超过2[25]。实现：对每个任职记录求分，按学年分组，每年多职只取最高，再累计各年但cap=2。
- 参军入伍：查服役年数y，一年以上加1，两年以上加2，max2。 
- 体育比赛：根据赛事级别和名次给予团队分值，然后个人=团队值1/3[44][45]。这里规则可配置：比如国际冠军8、国际亚军6.5...国家冠军5等[44]。实现：比赛记录有级别（国际/国家）和名次，查表得总分t，若个人项目则p=t/3，团队项目则按团队成员平均=t/团队人数。每人取最高一项，不累加。
5. 特殊学术专长：若学生标记特殊专长通过，则直接令学术专长成绩=15分，不再受上述分项总和影响[9]。实现上，可在计算总分时先判断标志位。算法模块设计考虑了灵活性和准确性，必要时可用单元测试验证各种组合情形确保符合文件精神。
4.3 界面低保真原型设计
在进入详细实现前，我们绘制了一系列低保真原型草图以规划界面布局和交互。这些线框图涵盖主要页面：
- 学生成果上传页原型：页面顶部有下拉菜单选择成果类型（论文/竞赛等），选择不同类型下方出现对应的表单字段集。右侧有“上传文件”按钮，文件上传后显示缩略图或文件名。表单字段如“成果名称”、“级别”、“获奖等级”等，某些字段系统可根据上传文件自动填写（OCR结果）并高亮提示，例如证书中的赛事名称自动填入“竞赛名称”栏，学生需确认或修正。表单底部有“添加此成果”按钮，点击后将在页面下方的成果列表中新增一条项显示简要信息和初步得分。学生可重复此过程添加多项成果。低保真图以简单框线表示这些区域和流程。 
- 学生加分明细页原型：以卡片或表格形式列出各加分项。每项显示类别图标+描述+分值，例如：“📄 论文：《题目》 (CCF A类) —— 8.0 分”。点开可展开看到详细依据：刊物XX为A类基准10分作者排名系数80%=8分，等等。页面底部汇总出学术专长小计、综合表现小计和总分。右上角显示学生当前排名（若公开）。设计一个“提交审核”大按钮。在原型上用矩形和文字标注各部分。
- 管理员审核列表页原型：一个表格列出所有待审核申请，列头包括姓名、专业、学业成绩、专长分、综合分、总分、状态。每行后有一个“审核”操作按钮。顶部有筛选条件如专业选择、状态过滤。低保真图示意一个表格和筛选栏布局。
- 管理员审核详情页原型：分为左右两栏：左侧列出该生信息和总分，右侧分段列出每类成果。每条成果项右侧有审核控件：如一个✔通过/✖退回开关，一个可编辑的分值或类别下拉框。如果管理员发现问题可修改类别值或点退回标记此条无效。顶端有“全部退回/通过”大按钮，也可逐条操作。底部有“保存审核结果”按钮。原型用盒子表示分区，用列表表示条目，交互按钮用符号标记位置。 
- 排名公示页原型：*简单列表显示拟录取学生名单和成绩。由于涉及隐私，面向学生公示的可能只显示排名和部分成绩。原型展示按名次列出匿名化（如学号后四位）及综合分。
低保真原型保证了我们在开发前统一前后端对于界面数据的需求，对应每个界面明确需要提供的API数据结构。这有助于接口设计，例如审核详情页需要后端提供学生所有成果及评分细节的接口，公示页需要提供排名数据接口等。界面原型详细内容会在附录提供，以指导前端开发。
4.4 综合成绩计算与推免细则算法设计
本部分前面已讨论算法思路，下面补充说明实现细节和性能考虑。我们计划将评分算法封装在ScoreCalculator类中，提供类似calculateTotalScore(studentId)的方法。调用时，该方法会从数据库拉取该学生的所有成果记录和学业成绩，然后按类别分别调用子过程：
- calcAcademicScore(record)：根据成绩排名或GPA计算80%部分的分值。具体可能需要将专业内排名第p的学生折算为相应基准分，可按照学院规则（可能以最高绩点为满分100转换）。
- calcResearchScore(achievements)：遍历学术专长类成果（论文、专利、竞赛、创新等），分别调用calcPaperScore(list<PaperDetail>)等函数算分，然后求和并截断至15分满分。如果有specialTalent标记则直接返回15.0。
- calcPerformanceScore(achievements)：类似地处理综合表现类成果（荣誉、志愿、干部、体育等），各算出分值和总和，截断至5分。 - 将以上得分相加，得出推免综合成绩。保存结果或返回前端。
算法实现中大量用到了条件判断和数学计算，这需要充分单元测试覆盖。考虑可读性和调整方便，我们在代码中多使用Kotlin的数据类和函数式风格。比如竞赛规则可以用一个配置数据类表示，让代码逻辑更清晰。现代编程范式比如Stream流处理或Kotlin集合操作将用于聚合计算，如按年份分组荣誉用 honors.groupBy { it.year }，然后对每组取最大分。
语法糖的使用（如默认参数、命名参数）也能让计算调用简洁明了[22]。
此外，我们可能在规则复杂处增加注释说明对应校规条款，以方便以后维护者理解算法。
在效率方面，这些计算主要在学生提交或管理员查询时触发，数据量不大（每学生成果数个位到几十），计算耗时可忽略不计。即使全院几百学生一起算，总体也在毫秒级。真正可能影响性能的是外部API调用，但我们已规划将OCR在上传时就完成，评分时无需再调OCR，只利用已保存的文本信息。因此综合成绩计算不会成为系统瓶颈。
4.5 开发流
遵循Google等开源项目代码规范，充分利用Kotlin现代特性编写简洁可靠代码[46]。
采用GitHub Flow工作流程管理代码：开发过程中以main分支为常驻可部署分支，feature功能点创建独立分支开发，经过Pull Request代码审核后合并到main[47][48]。这种简化的分支模型降低协作门槛、加快交付，保证main分支始终稳定可用[47]。
搭配持续集成（CI）在每次合并时运行测试和构建，确保代码质量。项目最终部署采用Docker容器化：后端制作含Spring Boot应用的Docker镜像，前端也通过Nginx镜像提供静态文件，数据库可以使用官方MySQL镜像。通过Docker Compose将这些服务编排在一起，开发者和部署人员只需一条命令即可启动全部服务环境。这大大简化了部署流程，使应用具有良好的可移植性和一致性——开发时调试好的容器在生产环境也能一致运行[49][44]。
容器轻量级、高度可移植，部署管理便捷，在应用发布过程中节省大量时间和精力[44]。
综合应用以上技术和架构设计，本系统将能够高效地开发实现，并易于后续升级维护。
5 系统测试
5.1 测试定义
系统测试是保证产品质量和可靠性的重要步骤。针对“保研加分小助手”，我们将进行多个层次的测试工作，包括单元测试、集成测试、系统功能测试和验收测试等，以覆盖模块内部逻辑、模块之间接口以及整体业务流程的正确性。测试过程中遵循既定的测试方案和用例设计，力求发现并纠正各种缺陷，确保系统交付时满足需求和性能指标。
5.2 测试目的
通过充分的测试，验证系统在各种情况下的正确性、稳定性和健壮性。具体而言：
- 检查评分算法是否精确符合推免细则要求，对不同组合的输入都能输出正确分值，尤其极端情况（如分数上限、同一项多次计分）要验证无误。 
- 验证OCR/NLP集成功能，对常见格式的证书和论文能够正确提取信息并分类。如OCR结果不理想的场景，系统也能妥善处理（如标记人工审核）。
- 确认前端交互和后端API契合度，页面功能如上传、提交、审核操作在不同浏览器均正常，前后端数据交换格式正确无遗漏。 
- 测试权限控制和安全机制有效：未登录用户无法访问内页，学生不能访问管理员接口，数据操作符合权限设定。尝试恶意输入或越权访问应被系统拒绝或过滤。 
- 评估系统性能在模拟高并发时的表现，确保响应时间在可接受范围（如上传材料<5秒返回，成绩查询<2秒），没有发生崩溃或数据错乱。 
- 检验容器部署配置，确保在新环境下仍可正常使用，避免环境依赖问题。
5.3 测试方案
本项目的测试将分阶段进行，结合模块开发进度采取“先模块后集成，先功能后性能”的策略。具体计划如下：
- 单元测试（模块测试）：由开发人员在完成各后端模块编码后编写JUnit测试用例，对模块内部的方法逻辑进行验证。例如：
- ScoreCalculator各子函数的单元测试：模拟各种论文/竞赛输入组合，断言输出分值是否与手工计算相符，包括边界情况如多篇C类论文截尾等。
- OCRService的单元测试：提供若干样例图片（可用项目资料或合成）给OCR接口，检查返回文本包含预期的关键词（如证书中的比赛名称）。考虑到实际OCR调用会产生费用和不确定性，这里可使用模拟（Mock）服务方法，对返回结果进行仿真，以测试系统对结果的处理逻辑。
- Repository层测试：在H2内存数据库中执行实体的增删改查操作，确保ORM映射正确。如插入Achievement后能正确级联存取其Detail表信息等。
- 工具方法测试：比如解析字符串、计算志愿服务小时分值等独立函数都应有相应测试。单元测试要求覆盖核心逻辑的主要路径，预期覆盖率不低于80%。这些测试由持续集成工具运行，每次代码变更都验证通过才能合并。
- 集成测试：在若干核心模块联通后进行。例如在评分算法联通数据库和API后，编写集成测试用例：
- 模拟一个学生完整提交流程：使用Mock MVC或RestTemplate发POST请求创建用户、提交几项成果，然后调用计算接口，看数据库记录和返回总分是否正确。尤其要测试REST接口序列化和反序列化是否正常、字段是否缺失。
- 测试管理员审核流：先通过接口添加一个待审核成果，然后调用审核API修改分值或退回，检查数据库状态变化以及返回给学生的状态标志是否更新。
- 集成测试会使用测试配置（如专门的测试数据库），不影响正式数据。重点是发现模块交互层面的问题，如数据格式不匹配、事务不一致等。
- 功能测试（系统测试）：由专门测试人员或开发人员在测试环境上对整个系统功能进行端到端验证。依据需求文档逐条列出测试用例，例如：
- 注册登录功能：测试正常注册、重复注册、登录密码错误、权限分配等。预期：重复注册提示错误，密码错误拒绝登录，正确登录后角色对应页面显示。
- 材料上传和识别：准备不同类型的材料文件（扫描件、截图），在前端上传，看是否可以预览，提交后系统是否返回识别内容。如上传不支持格式（如超大PDF或无文字图片）系统应提示错误。对OCR错误率高的文件，检查系统有没有明显提示或标记。
- 加分计算：准备典型学生案例：如某学生有2篇A类论文+1篇C类+国赛一等奖+校级荣誉等，手工算出他应得分，然后用系统填入检查总分。对每类规则设计一案例测试，如团队赛5人每个成员得分是否正确，荣誉称号两年多奖只算每年最高等。
- 审核流程：以管理员账号登录，查看待审学生列表，随机点入一条对比学生端填的信息，看是否一致。尝试修改一项成果分类，保存后查看学生端是否反映变化。测试退回功能：管理员退回一申请，学生登录应能看到退回原因并重新提交，再管理员通过。
- 排名与公示：填入多名学生数据（可写脚本批量造数据或从Excel导入），然后用系统排序，人工核对排序正确性。测试名额限制的场景：如专业名额少于申请数时系统只勾选前几名并标识其入选，其余为候补或未入选。公示页面查看不同权限展示的信息正确。
- 异常处理：包括网络中断模拟（上传途中断开，系统应有重试或提示）、服务器异常（模拟抛出未捕获异常，前端是否有友好错误提示）。还包括安全测试：登陆普通学生，尝试手工访问管理员URL，应该被重定向或403禁止；尝试修改别人的数据，系统应忽略或报错。
功能测试过程中，记录每个用例预期和实际结果，对于失败的用例进行Debug修复，然后再回归测试直至全部通过。功能测试确保系统在业务层面满足需求，没有遗漏功能点。
- 验收测试：在开发团队自测通过后，邀请实际用户代表（学院负责老师和部分学生）进行验收测试。模拟真实运行环境，让他们使用系统完成一次“假想的推免评审”，收集反馈。验收测试主要考察系统满足需求的程度和用户体验，如界面易用性、操作流程合理性。用户提出的问题（比如某提示文案不清、某操作繁琐等）记录下来进一步改进。验收测试通过的标志是用户认可系统功能满足最初提出的所有要求，且体验达标，没有阻碍使用的重大问题。
- 性能与压力测试：考虑系统用户规模有限，此项可适当简化。我们会使用工具（如JMeter或LoadRunner）模拟一定并发的操作，特别是对上传接口、计算接口进行压力测试。目标是确保在50人同时提交的情况下，系统响应依然及时，不会崩溃。重点观察服务器CPU、内存占用，数据库连接数等，调整必要的配置（如线程池大小、连接池）保证性能稳定。由于关键外部依赖OCR服务可能限制并发，我们也测试当连续多次上传时是否被限流，以及我们的排队策略是否生效（例如顺序处理或提示用户稍等）。
测试结果评估与改进：在整个测试过程中产生的Bug和问题将记录在Issue跟踪系统。我们会采用严重级别分类（高：功能错误或数据错误，需立刻修复；中：非预期行为但不影响主流程；低：UI文字瑕疵等）。团队会集中时间逐项修复，并进行针对性的复测。测试是一个反复迭代过程，直至主要功能完全正确、次要问题极少且可接受。最后将输出测试报告，包括测试覆盖率统计、通过的用例数、发现的问题列表及解决情况等。只有当测试报告表明系统达到预期质量标准时，才能进入上线部署阶段。通过严格的测试，我们有信心系统上线后能稳定可靠运行，准确地为推免评选服务。
6 工作计划
6.1 团队成员与岗位分工
本项目团队由4名成员组成，包括项目经理兼架构师1人，前端开发工程师1人，后端开发工程师1人，DevOps和测试负责1人。各成员具体职责分工如下：
- 项目经理/架构师（1人）：由软件工程资深导师担任，负责总体方案设计和技术决策，包括需求分析统筹、架构设计、核心代码审核等。同时协调团队进度，组织会议和把控项目质量。
- 前端开发工程师（1人）：负责完成所有Vue前端界面的开发与测试。一人侧重公共组件、状态管理和主要页面开发，另一人侧重表单细节、样式优化和前端与后端接口联调。共同确保前端代码符合Vue3最佳实践，UI美观且兼容各主流浏览器。
- 后端开发工程师（1人）：使用Kotlin完成服务器端编码。一人主要负责用户管理、安全和成果加分逻辑实现；另一人主要负责OCR/NLP集成、管理员审核流程和数据库设计。两者相互review代码，保持代码风格一致。后端团队也负责编写单元测试，保证核心算法正确。
- DevOps工程师/测试负责人（1人）：制定CI/CD流程，编写Docker部署脚本，配置Nginx等运维相关工作。同时牵头制定测试计划，组织成员进行测试，用自动化测试工具模拟负载，收集性能数据。该角色确保开发与部署、测试无缝衔接。
6.2 阶段安排
项目从启动到上线预计历时约4个月，划分为5个主要阶段，每阶段里程碑如下： 
1. 需求调研与分析阶段（第1-2周）：团队研读学院推免细则，走访教务老师和学生代表，完善需求文档和规格说明。输出：需求规格说明书（包括Use Case、用户故事）、初步领域模型。里程碑：需求评审通过。
2. 架构设计与原型阶段（第3-4周）：架构师主导编制总体设计文档，包括系统架构图、数据库ER图、关键模块设计方案。前端进行低保真原型设计并与用户确认UI布局。后端规划主要类和接口。输出：详细设计说明书，原型截图。里程碑：设计方案评审通过。 
3. 开发实现阶段（第5-12周）：分两个Sprint实施：
- Sprint 1（第5-8周）：重点攻克核心功能。后端完成用户认证、成果模型和加分计算模块，前端完成基本页面（登录、填报、查看分数）。在Sprint末进行一次可运行版本演示：学生可以提交成果并看到计算分数（虽未完善所有规则但主流程跑通）。
- Sprint 2（第9-12周）：完善全部功能和边界情况。实现管理员端所有操作，接入实际OCR/NLP服务，处理各种规则细节。前端补齐管理界面和优化交互体验。第12周末，系统功能全部完成并经过初步自测。里程碑：功能开发完毕。
 4. 测试与调整阶段（第13-15周）：测试负责人组织团队进行全面测试（单元测试、集成测试、用户验收测试）。记录问题清单，进入修复-回归循环。与用户开碰头会讨论验收结果，如有新需求变更也评估处理。持续完善直至测试通过。并在此阶段编写用户操作手册、部署文档等。里程碑：用户验收通过。 
5. 部署上线阶段（第16周）：准备生产环境服务器，安装Docker和Nginx等。DevOps工程师编写Docker Compose脚本，将前端、后端、数据库服务容器化部署[44]。做最后线上验证。然后在学院内部正式发布系统通知学生使用。在推免工作实际进行过程中，安排专人技术支持，及时处理可能出现的问题。
各阶段之间适当有重叠，采用并行协作：比如设计阶段后期前端即可开始编写组件库、后端可搭建项目骨架；测试阶段开发也可能并行修改Bug。通过甘特图形式，我们明确各任务的起止时间和依赖关系，确保关键路径任务按时完成。整个日程安排考虑了一定缓冲时间（约2周）以防不可预见延误，如政策变动或技术难题。
6.3 任务拆分与成员分工细化
在执行过程中，我们将大任务进一步细化为具体可管理的子任务，并由相应成员领取。举例如下： 
- 需求阶段：
  - N1: 收集学院推免规则文档（负责人：项目经理，1天） 
  - N2: 采访学工办老师获取手工流程细节（负责人：项目经理，2天） 
  - N3: 编写需求规格初稿并内部评审（负责人：全员参与，3天） 
- 设计阶段： 
  - D1: 数据库ER模型设计（负责人：后端A，2天） 
  - D2: 前端页面流程和框架搭建（负责人：前端A，3天） 
  - D3: 外部API接口选型与方案（负责人：后端B，研究Aliyun/Tencent API，2天）
  - D4: 完善详细设计文档（负责人：项目经理整理，全员review，3天）
- 开发阶段（Sprint 1为例）：
  - F1: 搭建Spring Boot工程结构和公共配置（后端A，1天） 
  - F2: 实现JWT登录认证模块（后端A，2天）
  - F3: 构建Vue项目架子，集成Element Plus和Pinia（前端A，2天）
  - F4: 数据库表迁移脚本和JPA实体定义（后端B，2天）
  - F5: 基础前端页面（登录/注册、学生主页框架）（前端B，3天）
  - F6: 成果提交API及加分计算初步（后端B，4天） 
  - F7: 前后端联调成果提交功能（前端A+后端B联合调试，2天） 
  - F8: ...（其他功能继续细分） 
- 测试阶段：
  - T1: 编写单元测试用例集（后端A负责算法部分、后端B负责API部分，各2天）
  - T2: 准备测试数据和场景（测试负责人，1天） 
  - T3: 执行功能测试用例，记录结果（全员参与，2天）
  - T4: 修复Bug清单（按模块指派给相应开发，若干天，视Bug情况循环） 
  - T5: 性能测试实施（测试负责人，1天）
  - T6: 组织用户验收测试会议（项目经理，0.5天）并收集反馈改进（1-2天） 
- 部署阶段： 
  - O1: 编写Dockerfile和Nginx配置（DevOps工程师，1天）  
  - O2: 配置服务器环境并部署演练（DevOps工程师，1天）
  - O3: 上线发布及运行监控（全员，0.5天）
  - O4: 项目总结和文档归档（项目经理主导，1天）
任务分配遵循每个成员的技能特长和工作量均衡。前端A/B大致均分模块（比如A负责管理员端界面，B负责学生端界面），后端A/B按领域分工（A负责安全和表现分, B负责学术成果和OCR集成等），但也在团队内部进行Code Review和互助以保证代码质量和进度同步。使用GitHub仓库管理代码，以Issue和Milestone跟踪任务完成情况，结合GitHub Flow模型管理分支[47]：每个任务开发一个分支，完成后提交Pull Request，由另一名成员评审通过再合并进主分支。通过这样的任务细分和流程，我们力求项目进展透明，可控按期完成。
7 参考文献
1. 厦门大学信息学院. 信息学院推荐优秀应届本科毕业生免试攻读研究生工作实施细则(2025年2月修订)[8][9]. 厦门大学信息学院通知公告, 2024.
2. 厦门大学信息学院. 信息学院推荐特殊学术专长优秀应届本科毕业生免试攻读研究生工作实施细则(2025年2月修订)[10][19]. 厦门大学信息学院通知公告, 2024.
3. 厦门大学信息学院. 2025年推荐优秀应届本科毕业生免试攻读研究生工作的通知[1][2]. 信息学院本科生通知公告, 2024-07-27.
4. QuarkApe. 综合素质测评系统 cqes4cs[12][5]. GitHub开源项目,      2025.
5. Pinia 官方文档. Pinia：符合直觉的Vue.js状态管理库[33][34].
6. Element Plus 阿里云开发者社区. Element Plus 是一套基于Vue3的组件库[35].
7. JetBrains Kotlin 博客. Kotlin简化后端开发，提高代码简洁和可维护性[22][46], 2025.
8. HelloMarsMan (CnBlogs). 领域驱动开发之代码工程结构：先业务，后技术[24][25], 2023.
9. Red Hat 官方. 容器的优势：轻量级、可移植、部署便捷[44].
10. Roy Fielding 等. 关于REST API版本化的讨论[41][42], 2013.
11. CSDN博客. GitHub Flow分支管理模型概述[47][48], 2025.
12. 霍聪聪, 倪胜巧 等. 新工科大学生综合素质智能评价系统的设计与实现[6][7]. 《计算机科学与应用》,      2021年.
13. 阿里云OCR产品文档. 阿里云文字识别（OCR）功能与场景[18].
14. 阿里云NLP产品文档. 自然语言处理基础能力：实体抽取、文本分类[15].
15. Google Cloud 文档. 容器 vs 虚拟机：容器易于管理和部署[44].

---
[1] [2] [8] [9] [10] [19] [20] [28] [29] [30] 信息学院（特色化示范性软件学院）关于做好2025年推荐优秀应届本科毕业生免试攻读研究生工作的通知-厦门大学信息学院
https://informatics.xmu.edu.cn/info/1024/39819.htm
[3] [39] SpringBoot + VUE技术架构 流程图模板_ProcessOn思维导图、流程图
https://www.processon.com/view/5cffb43ce4b0f1ac03705ae4
[4] [14] [27] 厦门大学信息学院（特色化示范性软件学院）关于2026年招收推荐免试研究生（含直博生）预报名的通知-厦门大学信息学院
https://informatics.xmu.edu.cn/info/1050/47240.htm
[5] [12] [13] [26] [43] GitHub - quarkape/cqes4cs: 〖毕业设计：综合素质测、评、管系统〗自定义配置加分规则、自动计分，助力高校更好的开展学生综合素质评价工作。建议拉取v1.1分支，该分支修改了重要的bug，且以后可能会维护更新。
https://github.com/quarkape/cqes4cs
[6] [7] [11]  新工科大学生综合素质智能评价系统的设计与实现 
https://www.hanspub.org/journal/paperinformation?paperid=42302
[15] [21] 文本情感分析api - 阿里云
https://www.aliyun.com/sswd/3103498-1.html
[16] 调用文档智能解析API_文档智能(Document Mind) - 阿里云文档
https://help.aliyun.com/zh/document-mind/developer-reference/docstructure
[17] [18] 文字识别(OCR)-阿里云帮助中心
https://help.aliyun.com/zh/ocr/
[22] [36] [46] 加强后端开发中的 Kotlin：与 Spring 建立战略合作伙伴关系 | The Kotlin Blog
https://blog.jetbrains.com/zh-hans/kotlin/2025/06/strategic-partnership-with-spring/
[23] 容器入门指南：定义、优势、用途及安全性
https://www.redhat.com/zh-cn/topics/containers
[24] [25] [38] [40] 领域驱动开发4-代码工程结构 - HelloMarsMan - 博客园
https://www.cnblogs.com/ToTigerMountain/articles/18656690
[31] [32] 系统架构图_前后端分离_springboot_vue 流程图模板_ProcessOn思维导图、流程图
https://www.processon.com/view/6603f0b598e2b2744cc6511a
[33] [34] Pinia | The intuitive store for Vue.js
https://pinia.vuejs.org/zh/
[35] VUE.js element plus-阿里云
https://cn.aliyun.com/sswb/689697.html?from_alibabacloud=
[37] DDD领域驱动设计实战-分层架构及代码目录结构 - 腾讯云
https://cloud.tencent.com/developer/article/2181065
[41] [42] Just say no - to versioning APIs — Reda
https://www.hmeid.com/blog/just-say-no-to-versioning
[44] [45] [49] 容器与虚拟机 | Google Cloud
https://cloud.google.com/discover/containers-vs-vms?hl=zh-CN
[47] [48] 史上最全分支管理模型总结（GitHub Flow）-CSDN博客
https://blog.csdn.net/SUWEISUWEISUWEI/article/details/139039458
